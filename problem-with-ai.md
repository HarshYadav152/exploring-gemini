1. Hallicunations --> model seems itself confident on a reasoning but it is completely wrong.
2. Bias --> due to trained data which might be biased towards something it will also reflect in model reasoning
3. Prompt Injection --> injecting malicious line of prompt to reveal sensitive information (security vulnerability)

### Always double check AI output.